{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVqcPmVBD6xR"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/krasserm/bayesian-machine-learning/blob/dev/bayesian-optimization/bayesian_optimization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-eLKRYHD6xS",
    "outputId": "7c784db6-68f2-49e1-8492-5fac8189a455"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Check if notebook is running in Google Colab\n",
    "    import google.colab\n",
    "    # Get additional files from Github\n",
    "    !wget https://raw.githubusercontent.com/krasserm/bayesian-machine-learning/dev/bayesian-optimization/bayesian_optimization_util.py\n",
    "    # Install additional dependencies\n",
    "    !pip install scikit-optimize==0.5.2\n",
    "    !pip install GPy==1.9.8\n",
    "    !pip install GPyOpt==1.2.1\n",
    "    !pip install xgboost==0.90\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_approximation(gpr, X, Y, X_sample, Y_sample, X_next=None, show_legend=False):\n",
    "    mu, std = gpr.predict(X, return_std=True)\n",
    "    plt.fill_between(X.ravel(), \n",
    "                     mu.ravel() + 1.96 * std, \n",
    "                     mu.ravel() - 1.96 * std, \n",
    "                     alpha=0.1) \n",
    "    plt.plot(X, Y, 'y--', lw=1, label='Noise-free objective')\n",
    "    plt.plot(X, mu, 'b-', lw=1, label='Surrogate function')\n",
    "    plt.plot(X_sample, Y_sample, 'kx', mew=3, label='Noisy samples')\n",
    "    if X_next:\n",
    "        plt.axvline(x=X_next, ls='--', c='k', lw=1)\n",
    "    if show_legend:\n",
    "        plt.legend()\n",
    "\n",
    "def plot_acquisition(X, Y, X_next, show_legend=False):\n",
    "    plt.plot(X, Y, 'r-', lw=1, label='Acquisition function')\n",
    "    plt.axvline(x=X_next, ls='--', c='k', lw=1, label='Next sampling location')\n",
    "    if show_legend:\n",
    "        plt.legend()    \n",
    "        \n",
    "def plot_convergence(X_sample, Y_sample, n_init=2):\n",
    "    plt.figure(figsize=(12, 3))\n",
    "\n",
    "    x = X_sample[n_init:].ravel()\n",
    "    y = Y_sample[n_init:].ravel()\n",
    "    r = range(1, len(x)+1)\n",
    "    \n",
    "    x_neighbor_dist = [np.abs(a-b) for a, b in zip(x, x[1:])]\n",
    "    y_max_watermark = np.maximum.accumulate(y)\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(r[1:], x_neighbor_dist, 'bo-')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.title('Distance between consecutive x\\'s')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(r, y_max_watermark, 'ro-')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Best Y')\n",
    "    plt.title('Value of best selected sample')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grz-28IID6xV"
   },
   "source": [
    "# Bayesian optimization\n",
    "\n",
    "## Introduction \n",
    "\n",
    "Many optimization problems in machine learning are black box optimization problems where the objective function $f(\\mathbf{x})$ is a black box function<sup>[1][2]</sup>. We do not have an analytical expression for $f$ nor do we know its derivatives. Evaluation of the function is restricted to sampling at a point $\\mathbf{x}$ and getting a possibly noisy response. \n",
    "\n",
    "If $f$ is cheap to evaluate we could sample at many points e.g. via grid search, random search or numeric gradient estimation. However, if function evaluation is expensive e.g. tuning hyperparameters of  a deep neural network, probe drilling for oil at given geographic coordinates or evaluating the effectiveness of a drug candidate taken from a chemical search space then it is important to minimize the number of samples drawn from the black box function $f$.\n",
    "\n",
    "This is the domain where Bayesian optimization techniques are most useful. They attempt to find the global optimimum in a minimum number of steps. Bayesian optimization incorporates prior belief about $f$ and updates the prior with samples drawn from $f$ to get a posterior that better approximates $f$. The model used for approximating the objective function is called *surrogate model*. Bayesian optimization also uses an *acquisition function* that directs sampling to areas where an improvement over the current best observation is likely.\n",
    "\n",
    "### Surrogate model\n",
    "\n",
    "A popular surrogate model for Bayesian optimization are [Gaussian processes](https://en.wikipedia.org/wiki/Gaussian_process) (GPs). I wrote about Gaussian processes in a [previous post](https://krasserm.github.io/2018/03/19/gaussian-processes/). If you are not familiar with GPs I recommend reading it first. GPs define a prior over functions and we can use them to incorporate prior beliefs about the objective function (smoothness, ...). The GP posterior is cheap to evaluate and is used to propose points in the search space where sampling is likely to yield an improvement. \n",
    "\n",
    "### Acquisition functions\n",
    "\n",
    "Proposing sampling points in the search space is done by acquisition functions. They trade off exploitation and exploration. Exploitation means sampling where the surrogate model predicts a high objective and exploration means sampling at locations where the prediction uncertainty is high. Both correspond to high acquisition function values and the goal is to maximize the acquisition function to determine the next sampling point. \n",
    "\n",
    "\n",
    "\n",
    "More formally, the objective function $f$ will be sampled at $\\mathbf{x}_t = \\mathrm{argmax}_{\\mathbf{x}} u(\\mathbf{x} \\lvert \\mathcal{D}_{1:t-1})$ where $u$ is the acquisition function and $\\mathcal{D}_{1:t-1} = \\{(\\mathbf{x}_1, y_1),...,(\\mathbf{x}_{t-1}, y_{t-1})\\}$ are the $t-1$ samples drawn from $f$ so far. Popular acquisition functions are *maximum probability of improvement* (MPI), *expected improvement* (EI) and *upper confidence bound* (UCB)<sup>[1]</sup>. In the following, we will use the expected improvement (EI) which is most widely used and described further below. \n",
    "\n",
    "### Optimization algorithm\n",
    "\n",
    "The Bayesian optimization procedure is as follows. For $t = 1,2,...$ repeat:\n",
    "\n",
    "- Find the next sampling point $\\mathbf{x}_{t}$ by optimizing the acquisition function over the GP: $\\mathbf{x}_t = \\mathrm{argmax}_{\\mathbf{x}} u(\\mathbf{x} \\lvert \\mathcal{D}_{1:t-1})$\n",
    "- Obtain a possibly noisy sample $y_t = f(\\mathbf{x}_t) + \\epsilon_t$ from the objective function $f$.\n",
    "- Add the sample to previous samples $\\mathcal{D}_{1:t} = \\{\\mathcal{D}_{1:t-1}, (\\mathbf{x}_t,y_t)\\}$ and update the GP.\n",
    "\n",
    "### Expected improvement\n",
    "\n",
    "Expected improvement is defined as\n",
    "\n",
    "$$\\mathrm{EI}(\\mathbf{x}) = \\mathbb{E}\\max(f(\\mathbf{x}) - f(\\mathbf{x}^+), 0)\\tag{1}$$\n",
    "\n",
    "where $f(\\mathbf{x}^+)$ is the value of the best sample so far and $\\mathbf{x}^+$ is the location of that sample i.e. $\\mathbf{x}^+ = \\mathrm{argmax}_{\\mathbf{x}_i \\in \\mathbf{x}_{1:t}} f(\\mathbf{x}_i)$. The expected improvement can be evaluated analytically under the GP model<sup>[3]</sup>:\n",
    "\n",
    "$$\n",
    "\\mathrm{EI}(\\mathbf{x}) =\n",
    "\\begin{cases}\n",
    "(\\mu(\\mathbf{x}) - f(\\mathbf{x}^+) - \\xi)\\Phi(Z) + \\sigma(\\mathbf{x})\\phi(Z)  &\\text{if}\\ \\sigma(\\mathbf{x}) > 0 \\\\\n",
    "0 & \\text{if}\\ \\sigma(\\mathbf{x}) = 0\n",
    "\\end{cases}\\tag{2}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "Z =\n",
    "\\begin{cases}\n",
    "\\frac{\\mu(\\mathbf{x}) - f(\\mathbf{x}^+) - \\xi}{\\sigma(\\mathbf{x})} &\\text{if}\\ \\sigma(\\mathbf{x}) > 0 \\\\\n",
    "0 & \\text{if}\\ \\sigma(\\mathbf{x}) = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\mu(\\mathbf{x})$ and $\\sigma(\\mathbf{x})$ are the mean and the standard deviation of the GP posterior predictive at $\\mathbf{x}$, respectively. $\\Phi$ and $\\phi$ are the CDF and PDF of the standard normal distribution, respectively. The first summation term in Equation (2) is the exploitation term and second summation term is the exploration term.\n",
    "\n",
    "Parameter $\\xi$ in Equation (2) determines the amount of exploration during optimization and higher $\\xi$ values lead to more exploration. In other words, with increasing $\\xi$ values, the importance of improvements predicted by the GP posterior mean $\\mu(\\mathbf{x})$ decreases relative to the importance of potential improvements in regions of high prediction uncertainty, represented by large $\\sigma(\\mathbf{x})$ values. A recommended default value for $\\xi$ is $0.01$.\n",
    "\n",
    "With this minimum of theory we can start implementing Bayesian optimization. The next section shows a basic implementation with plain NumPy and SciPy, later sections demonstrate how to use existing libraries. Finally, Bayesian optimization is used to tune the hyperparameters of a tree-based regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCcITplXD6xW"
   },
   "source": [
    "## Implementation with NumPy and SciPy\n",
    "\n",
    "In this section, we will implement the acquisition function and its optimization in plain NumPy and SciPy and use scikit-learn for the Gaussian process implementation. Although we have an analytical expression of the optimization objective `f` in the following example, we treat is as black box and iteratively approximate it with a Gaussian process during Bayesian optimization. Furthermore, samples drawn from the objective function are noisy and the noise level is given by the `noise` variable. Optimization is done within given `bounds`. We also assume that there exist two initial samples in `X_init` and `Y_init`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "X0AkEhtVD6xX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "bounds = np.array([[0.0, 2.0]])\n",
    "noise = 0.2\n",
    "\n",
    "def f(X, noise=noise):\n",
    "    return abs(-np.sin(3*X) - X**2 + 0.7*X + noise * np.random.randn(*X.shape))\n",
    "\n",
    "X_init = np.array([[0.5], [1.1]])\n",
    "Y_init = f(X_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfN-DOvWD6xZ"
   },
   "source": [
    "The following plot shows the noise-free objective function, the amount of noise by plotting a large number of samples and the two initial samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "XpZfvkblD6xa",
    "outputId": "01cd36c0-8435-4958-82a4-8d9cdf42f9ba"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dense grid of points within bounds\n",
    "X = np.arange(bounds[:, 0], bounds[:, 1], 0.01).reshape(-1, 1)\n",
    "\n",
    "# Noise-free objective function values at X \n",
    "Y = f(X,0)\n",
    "\n",
    "# Plot optimization objective with noise level \n",
    "plt.plot(X, Y, 'y--', lw=2, label='Noise-free objective')\n",
    "plt.plot(X, f(X), 'bx', lw=1, alpha=0.1, label='Noisy samples')\n",
    "plt.plot(X_init, Y_init, 'kx', mew=3, label='Initial samples')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwXJITtlD6xe"
   },
   "source": [
    "Goal is to find the global optimum on the left in a small number of steps. The next step is to implement the acquisition function defined in Equation (2) as `expected_improvement` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RCR5H_rDD6xe"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def expected_improvement(X, X_sample, Y_sample, gpr, xi=0.01):\n",
    "    '''\n",
    "    Computes the EI at points X based on existing samples X_sample\n",
    "    and Y_sample using a Gaussian process surrogate model.\n",
    "    \n",
    "    Args:\n",
    "        X: Points at which EI shall be computed (m x d).\n",
    "        X_sample: Sample locations (n x d).\n",
    "        Y_sample: Sample values (n x 1).\n",
    "        gpr: A GaussianProcessRegressor fitted to samples.\n",
    "        xi: Exploitation-exploration trade-off parameter.\n",
    "    \n",
    "    Returns:\n",
    "        Expected improvements at points X.\n",
    "    '''\n",
    "    mu, sigma = gpr.predict(X, return_std=True)\n",
    "    mu_sample = gpr.predict(X_sample)\n",
    "\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "    \n",
    "    # Needed for noise-based model,\n",
    "    # otherwise use np.max(Y_sample).\n",
    "    # See also section 2.4 in [...]\n",
    "    mu_sample_opt = np.max(mu_sample)\n",
    "\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu - mu_sample_opt - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "\n",
    "    return ei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6emZhkUkD6xh"
   },
   "source": [
    "We also need a function that proposes the next sampling point by computing the location of the acquisition function maximum. Optimization is restarted `n_restarts` times to avoid local optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VYFZVuQ9D6xh"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def propose_location(acquisition, X_sample, Y_sample, gpr, bounds, n_restarts=25):\n",
    "    '''\n",
    "    Proposes the next sampling point by optimizing the acquisition function.\n",
    "    \n",
    "    Args:\n",
    "        acquisition: Acquisition function.\n",
    "        X_sample: Sample locations (n x d).\n",
    "        Y_sample: Sample values (n x 1).\n",
    "        gpr: A GaussianProcessRegressor fitted to samples.\n",
    "\n",
    "    Returns:\n",
    "        Location of the acquisition function maximum.\n",
    "    '''\n",
    "    dim = X_sample.shape[1]\n",
    "    min_val = 1\n",
    "    min_x = None\n",
    "    \n",
    "    def min_obj(X):\n",
    "        # Minimization objective is the negative acquisition function\n",
    "        return -acquisition(X.reshape(-1, dim), X_sample, Y_sample, gpr)\n",
    "    \n",
    "    # Find the best optimum by starting from n_restart different random points.\n",
    "    for x0 in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, dim)):\n",
    "        res = minimize(min_obj, x0=x0, bounds=bounds, method='L-BFGS-B')        \n",
    "        if res.fun < min_val:\n",
    "            min_val = res.fun[0]\n",
    "            min_x = res.x           \n",
    "            \n",
    "    return min_x.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ld-qY13wD6xk"
   },
   "source": [
    "Now we have all components needed to run Bayesian optimization with the [algorithm](#Optimization-algorithm) outlined above. The Gaussian process in the following example is configured with a [MatÃ©rn kernel](http://scikit-learn.org/stable/modules/gaussian_process.html#matern-kernel) which is a generalization of the squared exponential kernel or RBF kernel. The known noise level is configured with the `alpha` parameter. \n",
    "\n",
    "Bayesian optimization runs for 10 iterations. In each iteration, a row with two plots is produced. The left plot shows the noise-free objective function, the surrogate function which is the GP posterior predictive mean, the 95% confidence interval of the mean and the noisy samples obtained from the objective function so far. The right plot shows the acquisition function. The vertical dashed line in both plots shows the proposed sampling point for the next iteration which corresponds to the maximum of the acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W0IHlaUhD6xl",
    "outputId": "6cb99202-efb8-4309-937e-eaefb2cae2f2",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`f0` passed has more than 1 dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d604383687d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Obtain next sampling point from the acquisition function (expected_improvement)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mX_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpropose_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_improvement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# Obtain next noisy sample from the objective function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-b39220cdbdf3>\u001b[0m in \u001b[0;36mpropose_location\u001b[1;34m(acquisition, X_sample, Y_sample, gpr, bounds, n_restarts)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Find the best optimum by starting from n_restart different random points.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx0\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbounds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_restarts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'L-BFGS-B'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mmin_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fajar\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    615\u001b[0m                                   **options)\n\u001b[0;32m    616\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[0;32m    618\u001b[0m                                 callback=callback, **options)\n\u001b[0;32m    619\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fajar\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0miprint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m     sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n\u001b[0m\u001b[0;32m    307\u001b[0m                                   \u001b[0mbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_bounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                                   finite_diff_rel_step=finite_diff_rel_step)\n",
      "\u001b[1;32mc:\\users\\fajar\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;31m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0m\u001b[0;32m    262\u001b[0m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fajar\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m# Hessian Evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fajar\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fajar\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_grad\u001b[1;34m()\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mngev\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m                 self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n\u001b[0m\u001b[0;32m     92\u001b[0m                                            **finite_diff_options)\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fajar\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scipy\\optimize\\_numdiff.py\u001b[0m in \u001b[0;36mapprox_derivative\u001b[1;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[0mf0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`f0` passed has more than 1 dimension.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: `f0` passed has more than 1 dimension."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern\n",
    "# from bayesian_optimization_util import plot_approximation, plot_acquisition\n",
    "\n",
    "# Gaussian process with MatÃ©rn kernel as surrogate model\n",
    "m52 = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)\n",
    "gpr = GaussianProcessRegressor(kernel=m52, alpha=noise**2)\n",
    "\n",
    "# Initialize samples\n",
    "X_sample = X_init\n",
    "Y_sample = Y_init\n",
    "\n",
    "# Number of iterations\n",
    "n_iter = 10\n",
    "\n",
    "plt.figure(figsize=(12, n_iter * 3))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "for i in range(n_iter):\n",
    "    # Update Gaussian process with existing samples\n",
    "    gpr.fit(X_sample, Y_sample)\n",
    "\n",
    "    # Obtain next sampling point from the acquisition function (expected_improvement)\n",
    "    X_next = propose_location(expected_improvement, X_sample, Y_sample, gpr, bounds)\n",
    "    \n",
    "    # Obtain next noisy sample from the objective function\n",
    "    Y_next = f(X_next, 0)\n",
    "    \n",
    "    # Plot samples, surrogate function, noise-free objective and next sampling location\n",
    "    plt.subplot(n_iter, 2, 2 * i + 1)\n",
    "    plot_approximation(gpr, X, Y, X_sample, Y_sample, X_next, show_legend=i==0)\n",
    "    plt.title(f'Iteration {i+1}')\n",
    "\n",
    "    plt.subplot(n_iter, 2, 2 * i + 2)\n",
    "    plot_acquisition(X, expected_improvement(X, X_sample, Y_sample, gpr), X_next, show_legend=i==0)\n",
    "    \n",
    "    # Add sample to previous samples\n",
    "    X_sample = np.vstack((X_sample, X_next))\n",
    "    Y_sample = np.vstack((Y_sample, Y_next))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zs-4KXE7D6xo"
   },
   "source": [
    "Note how the two initial samples initially drive search into the direction of the local maximum on the right side but exploration allows the algorithm to escape from that local optimum and find the global optimum on the left side. Also note how sampling point proposals often fall within regions of high uncertainty (exploration) and are not only driven by the highest surrogate function values (exploitation).\n",
    "\n",
    "A convergence plot reveals how many iterations are needed the find a maximum and if the sampling point proposals stay around that maximum i.e. converge to small proposal differences between consecutive steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "iKY1HzMbD6xo",
    "outputId": "d06c38a9-405c-4fb3-e7ae-bf46b5d633a9"
   },
   "outputs": [],
   "source": [
    "from bayesian_optimization_util import plot_convergence\n",
    "\n",
    "plot_convergence(X_sample, Y_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wHrWRf0D6xt"
   },
   "source": [
    "## Bayesian optimization libraries\n",
    "\n",
    "There are numerous Bayesian optimization libraries out there and giving a comprehensive overview is not the goal of this article. Instead, I'll pick two that I used in the past and show the minimum setup needed to get the previous example running.\n",
    "\n",
    "### Scikit-optimize\n",
    "\n",
    "[Scikit-optimize](https://scikit-optimize.github.io/) is a library for sequential model-based optimization that is based on [scikit-learn](http://scikit-learn.org/). It also supports Bayesian optimization using Gaussian processes. The API is designed around minimization, hence, we have to provide negative objective function values.  The results obtained here slightly differ from previous results because of non-deterministic optimization behavior and different noisy samples drawn from the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "EPs2ZFk0D6xu",
    "outputId": "463d53c2-0269-48fe-de90-221f832f8c94"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from skopt import gp_minimize\n",
    "from skopt.learning import GaussianProcessRegressor\n",
    "from skopt.learning.gaussian_process.kernels import ConstantKernel, Matern\n",
    "\n",
    "# Use custom kernel and estimator to match previous example\n",
    "m52 = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)\n",
    "gpr = GaussianProcessRegressor(kernel=m52, alpha=noise**2)\n",
    "\n",
    "r = gp_minimize(lambda x: -f(np.array(x))[0], \n",
    "                bounds.tolist(),\n",
    "                base_estimator=gpr,\n",
    "                acq_func='EI',      # expected improvement\n",
    "                xi=0.01,            # exploitation-exploration trade-off\n",
    "                n_calls=10,         # number of iterations\n",
    "                n_random_starts=0,  # initial samples are provided\n",
    "                x0=X_init.tolist(), # initial samples\n",
    "                y0=-Y_init.ravel())\n",
    "\n",
    "# Fit GP model to samples for plotting results\n",
    "gpr.fit(r.x_iters, -r.func_vals)\n",
    "\n",
    "# Plot the fitted model and the noisy samples\n",
    "plot_approximation(gpr, X, Y, r.x_iters, -r.func_vals, show_legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "V4wkWpsRD6xw",
    "outputId": "8a5c805a-900a-4b36-97c7-9a55dcf02427"
   },
   "outputs": [],
   "source": [
    "plot_convergence(np.array(r.x_iters), -r.func_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOE50dUWD6xy"
   },
   "source": [
    "## GPyOpt\n",
    "\n",
    "[GPyOpt](http://sheffieldml.github.io/GPyOpt/) is a Bayesian optimization library based on [GPy](https://sheffieldml.github.io/GPy/). The abstraction level of the API is comparable to that of scikit-optimize. The `BayesianOptimization` API provides a `maximize` parameter to configure whether the objective function shall be maximized or minimized (default). In version 1.2.1, this seems to be ignored when providing initial samples, so we have to negate their target values manually in the following example. Also, the built-in `plot_acquisition` and `plot_convergence` methods display the minimization result in any case. Again, the results obtained here slightly differ from previous results because of non-deterministic optimization behavior and different noisy samples drawn from the objective function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "nw7eG6bmD6xz",
    "outputId": "afdc06ca-a03d-4684-b115-9e016f997c10"
   },
   "outputs": [],
   "source": [
    "import GPy\n",
    "import GPyOpt\n",
    "\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "\n",
    "kernel = GPy.kern.Matern52(input_dim=1, variance=1.0, lengthscale=1.0)\n",
    "bds = [{'name': 'X', 'type': 'continuous', 'domain': bounds.ravel()}]\n",
    "\n",
    "optimizer = BayesianOptimization(f=f, \n",
    "                                 domain=bds,\n",
    "                                 model_type='GP',\n",
    "                                 kernel=kernel,\n",
    "                                 acquisition_type ='EI',\n",
    "                                 acquisition_jitter = 0.01,\n",
    "                                 X=X_init,\n",
    "                                 Y=-Y_init,\n",
    "                                 noise_var = noise**2,\n",
    "                                 exact_feval=False,\n",
    "                                 normalize_Y=False,\n",
    "                                 maximize=True)\n",
    "\n",
    "optimizer.run_optimization(max_iter=10)\n",
    "optimizer.plot_acquisition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "BijvcyfaD6x1",
    "outputId": "38eba955-fd9f-495e-b57a-4241c437b2cc"
   },
   "outputs": [],
   "source": [
    "optimizer.plot_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyQh_2XUD6x4"
   },
   "source": [
    "## Application\n",
    "\n",
    "This section demonstrates how to optimize the hyperparameters of an `XGBRegressor` with GPyOpt and how Bayesian optimization performance compares to random search. `XGBRegressor` is part of [XGBoost](https://xgboost.readthedocs.io/), a flexible and scalable gradient boosting library. `XGBRegressor` implements the scikit-learn estimator API and can be applied to regression problems. Regression is performed on a small [toy dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes) that is part of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJA7zajcD6x4",
    "outputId": "88106f7f-4ae6-417f-f093-d1b65f22c3fa"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "\n",
    "from scipy.stats import uniform\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load the diabetes dataset (for regression)\n",
    "X, Y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Instantiate an XGBRegressor with default hyperparameter settings\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "# and compute a baseline to beat with hyperparameter optimization \n",
    "baseline = cross_val_score(xgb, X, Y, scoring='neg_mean_squared_error').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Drndt95ND6x7"
   },
   "source": [
    "### Hyperparameter tuning with random search\n",
    "\n",
    "For hyperparameter tuning with random search, we use `RandomSearchCV` of scikit-learn and compute a cross-validation score for each randomly selected point in hyperparameter space. Results will be discussed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MXNSQDVMD6x7",
    "outputId": "78807e0d-d4f0-43cc-a1d7-b5d153d7f5af"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters to tune and their ranges\n",
    "param_dist = {\"learning_rate\": uniform(0, 1),\n",
    "              \"gamma\": uniform(0, 5),\n",
    "              \"max_depth\": range(1,50),\n",
    "              \"n_estimators\": range(1,300),\n",
    "              \"min_child_weight\": range(1,10)}\n",
    "\n",
    "rs = RandomizedSearchCV(xgb, param_distributions=param_dist, \n",
    "                        scoring='neg_mean_squared_error', n_iter=25)\n",
    "\n",
    "# Run random search for 25 iterations\n",
    "rs.fit(X, Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r26Ny-SXD6x-"
   },
   "source": [
    "### Hyperparameter tuning with Bayesian optimization\n",
    "\n",
    "To tune hyperparameters with Bayesian optimization we implement an objective function `cv_score` that takes hyperparameters as input and returns a cross-validation score. Here, we assume that cross-validation at a given point in hyperparameter space is deterministic and therefore set the `exact_feval` parameter of `BayesianOptimization` to `True`. Depending on model fitting and cross-validation details this might not be the case but we ignore that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TsOxKPSAD6x-",
    "outputId": "1183181a-add3-4e19-e534-066126d9c72e"
   },
   "outputs": [],
   "source": [
    "bds = [{'name': 'learning_rate', 'type': 'continuous', 'domain': (0, 1)},\n",
    "        {'name': 'gamma', 'type': 'continuous', 'domain': (0, 5)},\n",
    "        {'name': 'max_depth', 'type': 'discrete', 'domain': (1, 50)},\n",
    "        {'name': 'n_estimators', 'type': 'discrete', 'domain': (1, 300)},\n",
    "        {'name': 'min_child_weight', 'type': 'discrete', 'domain': (1, 10)}]\n",
    "\n",
    "# Optimization objective \n",
    "def cv_score(parameters):\n",
    "    parameters = parameters[0]\n",
    "    score = cross_val_score(\n",
    "                XGBRegressor(learning_rate=parameters[0],\n",
    "                              gamma=int(parameters[1]),\n",
    "                              max_depth=int(parameters[2]),\n",
    "                              n_estimators=int(parameters[3]),\n",
    "                              min_child_weight = parameters[4]), \n",
    "                X, Y, scoring='neg_mean_squared_error').mean()\n",
    "    score = np.array(score)\n",
    "    return score\n",
    "\n",
    "optimizer = BayesianOptimization(f=cv_score, \n",
    "                                 domain=bds,\n",
    "                                 model_type='GP',\n",
    "                                 acquisition_type ='EI',\n",
    "                                 acquisition_jitter = 0.05,\n",
    "                                 exact_feval=True, \n",
    "                                 maximize=True)\n",
    "\n",
    "# Only 20 iterations because we have 5 initial random points\n",
    "optimizer.run_optimization(max_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUhKtmxVD6yA"
   },
   "source": [
    "### Results\n",
    "\n",
    "On average, Bayesian optimization finds a better optimium in a smaller number of steps than random search and beats the baseline in almost every run. This trend becomes even more prominent in higher-dimensional search spaces. Here, the search space is 5-dimensional which is rather low to substantially profit from Bayesian optimization. One advantage of random search is that it is trivial to parallelize. Parallelization of Bayesian optimization is much harder and subject to research (see \\[4\\], for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "enTDlQ_BD6yB",
    "outputId": "5d6f0a1c-e057-4ab8-b489-32e227ae4b00"
   },
   "outputs": [],
   "source": [
    "y_rs = np.maximum.accumulate(rs.cv_results_['mean_test_score'])\n",
    "y_bo = np.maximum.accumulate(-optimizer.Y).ravel()\n",
    "\n",
    "print(f'Baseline neg. MSE = {baseline:.2f}')\n",
    "print(f'Random search neg. MSE = {y_rs[-1]:.2f}')\n",
    "print(f'Bayesian optimization neg. MSE = {y_bo[-1]:.2f}')\n",
    "\n",
    "plt.plot(y_rs, 'ro-', label='Random search')\n",
    "plt.plot(y_bo, 'bo-', label='Bayesian optimization')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Neg. MSE')\n",
    "plt.ylim(-5000, -3000)\n",
    "plt.title('Value of the best sampled CV score');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_F4guyDD6yD"
   },
   "source": [
    "## References\n",
    "\n",
    "\\[1\\] Eric Brochu, Vlad M. Cora, Nando de Freitas, [A Tutorial on Bayesian Optimization of Expensive Cost Functions](https://arxiv.org/abs/1012.2599).  \n",
    "\\[2\\] Jonas Mockus, [Application of Bayesian approach to numerical methods of global and stochastic optimization](https://link.springer.com/article/10.1007/BF01099263).  \n",
    "\\[3\\] Donald R. JonesMatthias SchonlauWilliam J. Welch, [Efficient Global Optimization of Expensive Black-Box Functions](https://link.springer.com/article/10.1023/A:1008306431147).  \n",
    "\\[4\\] Jialei Wang, Scott C. Clark, Eric Liu, Peter I. Frazier, [Parallel Bayesian Global Optimization of Expensive Functions](https://arxiv.org/abs/1602.05149).  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "bayesian_optimization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
